#### 5.1.3 Dimensionality reduction

_If some characters seem to be missing, it's because MathJax is not loaded correctly. Refreshing the page should fix it._

1. [E] Why do we need dimensionality reduction?
    
    *Answer:*
    Dimensionality reduction is a technique used to reduce the number of dimensions (i.e., variables or features) in a dataset while retaining as much information as possible. There are several reasons why we might want to do this:
    1. Simplicity: A large number of dimensions can make it difficult to visualize and analyze data. By reducing the number of dimensions, we can more easily understand and interpret the data.
    2. Computational efficiency: Working with high-dimensional data can be computationally expensive, especially when we want to perform machine learning tasks on the data. Reducing the dimensionality of the data can make it faster to train models and make predictions.
    3. Curse of dimensionality: As the number of dimensions increases, the amount of data we need to accurately model the data increases exponentially. This can make it difficult to find patterns in the data, and can lead to overfitting.
    4. Noise reduction: Some dimensions may contain a lot of noise or may be irrelevant to the task at hand. Dimensionality reduction can help remove these dimensions and improve the performance of machine learning models.
    5. Better data representation: Some dimensionality reduction techniques can find a new, lower-dimensional representation of the data that may be more meaningful and easier to understand than the original representation.
    Regenerate response
2. [E] Eigendecomposition is a common factorization technique used for dimensionality reduction. Is the eigendecomposition of a matrix always unique?

    *Answer:* The eigendecomposition of a matrix may not be unique. If any two or more eigenvectors share the same eigenvalue, then any set of orthogonal vectors lying in their span are also eigenvectors with that eigenvalue, and we could equivalently choose a Q using those eigenvectors. By convention, the entries are sorted in descending order. Under this convention, the eigendecomposition is unique only if the eigenvalues are unique.

    Reference: [Medium](https://sakshihmss.medium.com/eigen-value-decomposition-9d31f17a70fc)
3. [M] Name some applications of eigenvalues and eigenvectors.


    *Answer:* Eigenvalues and eigenvectors are mathematical concepts that have a wide range of applications in various fields, including:
    * Image processing: Eigenvalues and eigenvectors can be used to analyze the structure of images, to extract features from images, and to classify images.
    * Computer graphics: Eigenvalues and eigenvectors can be used to rotate and scale objects in computer graphics, and to perform other transformations on 3D models.
    * Natural language processing: Eigenvalues and eigenvectors can be used to represent the semantics of words and sentences, and to perform tasks such as text classification and document clustering.
    * Machine learning: Eigenvalues and eigenvectors can be used to perform dimensionality reduction, to improve the performance of machine learning algorithms, and to extract features from data.
    * Physics: Eigenvalues and eigenvectors are used in various areas of physics, such as quantum mechanics, where they are used to describe the behavior of particles and systems.
    * Engineering: Eigenvalues and eigenvectors are used in various engineering applications, such as structural analysis and control systems design.
    
    * Economics: Eigenvalues and eigenvectors are used in economics to analyze the stability of economic systems, and to predict the behavior of economic variables over time.

4. [M] We want to do PCA on a dataset of multiple features in different ranges. For example, one is in the range 0-1 and one is in the range 10 - 1000. Will PCA work on this dataset?


    *Answer:* No,Principal Component Analysis (PCA) is a technique for dimensionality reduction that is based on the linear combinations of the original variables. When applying PCA to a dataset, it is important to ensure that the features are on a similar scale, because the method is sensitive to the scale of the features.

    If the features in your dataset are on different scales, it is generally a good idea to scale the features before applying PCA. There are several methods for scaling the features, including standardization and normalization. Standardization involves centering the features at 0 and scaling them to have unit variance, while normalization involves scaling the features to have a range of 0 to 1.
    
    By scaling the features to a similar scale, you can ensure that PCA is able to capture the most important patterns in the data and reduce the dimensionality effectively. If the features are not scaled, the PCA transformation may be dominated by the features with a larger scale, which could result in a less informative or less useful representation of the data.

5. [H] Under what conditions can one apply eigendecomposition? What about SVD?
    *Answer:*
    Eigendecomposition: Eigendecomposition is a technique for decomposing a matrix into a product of a matrix of eigenvectors and a diagonal matrix of eigenvalues. It can be applied to any matrix that is square and has a full set of eigenvectors. In other words, the matrix must be diagonalizable.

    SVD: SVD is a technique for decomposing a matrix into the product of three matrices: a matrix of left singular vectors, a diagonal matrix of singular values, and a matrix of right singular vectors. It can be applied to any matrix, whether it is square or not.

    1. What is the relationship between SVD and eigendecomposition?


        *Answer:*
        The relationship between SVD and eigendecomposition is that SVD is a generalization of eigendecomposition. In other words, every matrix has a unique SVD, but not every matrix has a unique eigendecomposition.

    1. Whatâ€™s the relationship between PCA and SVD?


        *Answer:* [TODO:]Add more

        The relationship between PCA and SVD is that PCA can be implemented using SVD. PCA is a technique for finding the directions in a data set that maximize the variance, and it is often implemented by performing SVD on the data matrix. The left singular vectors of the SVD correspond to the principal components of the data, and the singular values of the SVD correspond to the variances of the data along the corresponding principal components.

6. [H] How does t-SNE (T-distributed Stochastic Neighbor Embedding) work? Why do we need it?


    *Answer:* 
    t-SNE (T-distributed Stochastic Neighbor Embedding) is a technique for dimensionality reduction that is particularly well-suited for visualizing high-dimensional data in a low-dimensional space. It works by constructing a probability distribution over pairs of high-dimensional data points, and minimizing the Kullback-Leibler divergence between the distribution and a similar distribution over the low-dimensional space.

    The t-SNE algorithm works in two steps:

    Compute pairwise affinities: The t-SNE algorithm starts by computing the pairwise affinities between all pairs of points in the high-dimensional space. The affinities are calculated using the Gaussian kernel, which measures the similarity between two points based on their distance in the high-dimensional space.

    Minimize the divergence: The t-SNE algorithm then tries to find a low-dimensional representation of the data such that the relative distances between the points in the low-dimensional space are as close as possible to the affinities in the high-dimensional space. This is done by minimizing the Kullback-Leibler divergence between the distribution in the low-dimensional space and the distribution in the high-dimensional space.

    We need t-SNE because it is often difficult to visualize high-dimensional data in a meaningful way. By reducing the dimensionality of the data to two or three dimensions, we can more easily visualize and understand the patterns and relationships in the data. t-SNE is particularly useful for visualizing complex, non-linear relationships in the data.


