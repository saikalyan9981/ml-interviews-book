#### 5.1.4 Calculus and convex optimization

_If some characters seem to be missing, it's because MathJax is not loaded correctly. Refreshing the page should fix it._

1. Differentiable functions
    1. [E] What does it mean when a function is differentiable?

        *Answer:* A function is differentiable at a point if it has a well-defined derivative at that point. This means that the rate of change of the function at that point exists and can be calculated using the derivative.

    1. [E] Give an example of when a function doesn’t have a derivative at a point.
    
        *Answer:* A classic example of a function that does not have a derivative at a point is the absolute value function at 0. Another example is the square root function at 0.

    1. [M] Give an example of non-differentiable functions that are frequently used in machine learning. How do we do backpropagation if those functions aren’t differentiable?
    
        *Answer:* ReLU (Rectified Linear Unit) is an example of a non-differentiable function that is frequently used in machine learning. At the point where the input is zero, the derivative of ReLU is undefined. However, we can still do backpropagation using sub-differentials, which is a generalized notion of a derivative that allows us to handle non-differentiable functions.
    
2. Convexity
    1. [E] What does it mean for a function to be convex or concave? Draw it.
    
        *Answer:* A function is convex if its graph is always above its tangent lines. A function is concave if its graph is always below its tangent lines. A convex function has a positive second derivative, while a concave function has a negative second derivative.


    1. [E] Why is convexity desirable in an optimization problem?
    
        *Answer:* Convexity is desirable in an optimization problem because it means that there is only one global minimum for the function. In non-convex optimization problems, there may be multiple local minima, which makes finding the global minimum more difficult.
    1. [M] Show that the cross-entropy loss function is convex.

        *Answer:* The cross-entropy loss function is defined as:

        $$L(y,\hat{y}) = -\sum_{i=1}^{m} y_ilog\hat{y_i} + (1-y_i)log(1-\hat{y_i})$$

        where $y$ and $\hat{y}$ are the true and predicted probabilities, respectively. Since the logarithm is a monotonically increasing function, this means that the cross-entropy loss function is a sum of convex functions and therefore is convex.
3. Given a logistic discriminant classifier:
    
    $$
        p(y=1|x) = \sigma (w^Tx)
    $$
    
    where the sigmoid function is given by:

    $$
        \sigma(z) = (1 + \exp(-z))^{-1}
    $$

    The logistic loss for a training sample $$x_i$$ with class label $$y_i$$ is given by:
    
    $$
        L(y_i, x_i;w) = -\log p(y_i|x_i)
    $$

    1. Show that $$p(y=-1|x) = \sigma(-w^Tx)$$.

        *Answer:*  $p(y=-1|x) = 1- p(y=1|x) = \sigma(-w^Tx)$
    1. Show that $$\Delta_wL(y_i, x_i; w) = -y_i(1-p(y_i|x_i))x_i$$.
        *Answer:* 
    1. Show that $$\Delta_wL(y_i, x_i; w)$$ is convex.
        *Answer:* 

4. Most ML algorithms we use nowadays use first-order derivatives (gradients) to construct the next training iteration.
    1. [E] How can we use second-order derivatives for training models?

        *Answer:* Second-order derivatives, also known as Hessian matrices, can be used to provide information about the curvature of a function. This information can be used to make the optimization process more efficient and robust by choosing a step size that takes into account the curvature of the function. This approach is known as second-order optimization.
    1. [M] Pros and cons of second-order optimization.

        *Answer:*
 
        Pros:

        Improved convergence: Second-order optimization can converge faster and more accurately than first-order optimization because it takes into account the curvature of the function.

        Robustness: Second-order optimization is more robust to poor initializations and noisy gradients because it considers the curvature of the function.

        Cons:

        Computational cost: The computation of second-order derivatives is more expensive than that of first-order derivatives. This can make second-order optimization infeasible for very large datasets or models.

        Memory requirements: The storage of the Hessian matrix can be very memory-intensive, making second-order optimization impractical for very large models.

    1. [M] Why don’t we see more second-order optimization in practice?
        *Answer:*

        Despite its benefits, second-order optimization is not widely used in practice because of its computational and memory costs. For very large datasets or models, the computation of second-order derivatives and the storage of the Hessian matrix can be infeasible. In these cases, first-order optimization algorithms, such as gradient descent, are preferred. Additionally, first-order optimization algorithms can often be highly effective, especially when combined with techniques such as mini-batch training and learning rate schedules.

5. [M] How can we use the Hessian (second derivative matrix) to test for critical points? 
    
    *Answer: *

    The Hessian (second derivative matrix) can be used to test for critical points by determining if the Hessian is positive definite or negative definite at that point. If the Hessian is positive definite, it indicates that the function is a local minimum at that point. If the Hessian is negative definite, it indicates that the function is a local maximum at that point

6. [E] Jensen’s inequality forms the basis for many algorithms for probabilistic inference, including Expectation-Maximization and variational inference.. Explain what Jensen’s inequality is.

    *Answer:* Jensen's inequality states that if a real-valued function $$f$$ is convex, then for any random variable $X$ with probability distribution $P$, $f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]$. This means that the expected value of a convex function is always greater than or equal to the value of the function at the expected value of its argument.

7. [E] Explain the chain rule.

    *Answer:* The chain rule is a basic concept in calculus that states that the derivative of a composed function $f(g(x))$ can be calculated by multiplying the derivative of $g(x)$ with the derivative of $f(x)$. In other words, $(f(g(x)))' = f'(g(x)) * g'(x)$. The chain rule is used to find the derivative of functions composed of several sub-functions.

    
8. [M] Let $$x \in R_n$$, $$L = crossentropy(softmax(x), y)$$ in which $$y$$ is a one-hot vector. Take the derivative of $$L$$ with respect to $$x$$.

    *Answer:*
    <!-- Using the chain rule, the derivative of the cross-entropy loss with respect to $x$ is given by:

    $$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial x} = \frac{\partial L}{\partial z} \frac{\partial}{\partial x} softmax(x)$$

    where $$z = softmax(x)$$. We have:

    $$\frac{\partial L}{\partial z_i} = \frac{\partial}{\partial z_i} -\sum_{k=1}^n y_k \log z_k = -\frac{y_i}{z_i}$$

    $$\frac{\partial}{\partial x_j} softmax(x) = \frac{\partial}{\partial x_j} \frac{e^{x_j}}{\sum_{k=1}^n e^{x_k}} = \frac{e^{x_j}(\sum_{k=1}^n e^{x_k} - e^{x_j})}{(\sum_{k=1}^n e^{x_k})^2} = z_j (\delta_{j, i} - z_i)$$

    where $\delta_{j, i}$ is the Kronecker delta symbol. Combining these two results, we have:

    $$\frac{\partial L}{\partial x_i} = -y_i (1 - z_i)$$ -->

    
9. [M] Given the function $$f(x, y) = 4x^2 - y$$ with the constraint $$x^2 + y^2 =1$$. Find the function’s maximum and minimum values.

    *Answer: *
    <!-- To find the maximum and minimum values of $$f(x,y)$$ with the constraint $$x^2 + y^2 = 1$$, we need to use a method called Lagrange multipliers.

    In this method, we consider a new function $$L(x, y, \lambda) = f(x,y) - \lambda (x^2 + y^2 - 1)$$, where $$\lambda$$ is the Lagrange multiplier.

    Setting the partial derivatives of $$L$$ with respect to $$x$$, $$y$$, and $$\lambda$$ to zero, we get:

    $$\begin{aligned} \frac{\partial L}{\partial x} &= \frac{\partial f}{\partial x} - 2\lambda x = 8x - 2\lambda x = 0 \ \frac{\partial L}{\partial y} &= \frac{\partial f}{\partial y} + 2\lambda y = -1 - 2\lambda y = 0 \ \frac{\partial L}{\partial \lambda} &= x^2 + y^2 - 1 = 0 \end{aligned}$$

    Solving for $$x$$ and $$y$$, we obtain:

    $$\begin{aligned} 8x - 2\lambda x &= 0 \ -1 - 2\lambda y &= 0 \end{aligned}$$

    $$\begin{aligned} x &= \frac{\lambda}{4} \ y &= -\frac{1}{2\lambda} \end{aligned}$$

    Plugging these values into $$x^2 + y^2 = 1$$, we get:

    $$\lambda^2 + \left(-\frac{2}{\lambda}\right)^2 = 1$$

    $$\lambda^4 - 4\lambda^2 - 4 = 0$$

    Solving for $$\lambda$$, we find that $$\lambda = \pm \sqrt{2}$$. Substituting $$\lambda$$ back into the expressions for $$x$$ and $$y$$, we get:

    $$\begin{aligned} x &= \pm \frac{\sqrt{2}}{4} \ y &= \mp \frac{\sqrt{2}}{2} \end{aligned}$$

    Finally, we substitute these values into $$f(x,y)$$ to find the maximum and minimum values:

    $$\begin{aligned} f(x,y) &= 4x^2 - y \ &= 4\left(\pm \frac{\sqrt{2}}{4}\right)^2 - \mp \frac{\sqrt{2}}{2} \ &= \pm \sqrt{2} \pm \frac{\sqrt{2}}{2} \ &= \pm \sqrt{2} + \frac{\sqrt{2}}{2} \end{aligned}$$

    So the maximum value of $$f(x,y)$$ is $$\sqrt{2} + \frac{\sqrt{2}}{2}$$ and the minimum value is $$-\sqrt{2} - \frac{\sqrt{2}}{2}$$. -->



    
----
> On convex optimization

Convex optimization is important because it's the only type of optimization that we more or less understand. Some might argue that since many of the common objective functions in deep learning aren't convex, we don't need to know about convex optimization. However, even when the functions aren't convex, analyzing them as if they were convex often gives us meaningful bounds. If an algorithm doesn't work assuming that a loss function is convex, it definitely doesn't work when the loss function is non-convex.

Convexity is the exception, not the rule. If you're asked whether a function is convex and it isn't already in the list of commonly known convex functions, there's a good chance that it isn't convex. If you want to learn about convex optimization, check out [Stephen Boyd's textbook](http://cs229.stanford.edu/section/cs229-cvxopt.pdf).

----
> On Hessian matrix

The Hessian matrix or Hessian is a square matrix of second-order partial derivatives of a scalar-valued function. 

Given a function $$f : ℝn → ℝ$$. If all second partial derivatives of f exist and are continuous over the domain of the function, then the Hessian matrix H of f is a square nn matrix such that: $$H_{ij}=\frac{\delta f}{\delta x_i\delta x_j}$$.

<center>
    <img src="images/image18.png" width="40%" alt="Hessian matrix" title="image_tooltip">
</center>

The Hessian is used for large-scale optimization problems within Newton-type methods and quasi-Newton methods. It is also commonly used for expressing image processing operators in image processing and computer vision for tasks such as blob detection and multi-scale signal representation.