### 7.1 Basics

1. [E] Explain supervised, unsupervised, weakly supervised, semi-supervised, and active learning.


    *Answer:*
    Supervised learning is a machine learning paradigm where the algorithm is trained on labeled examples to learn a function that maps inputs to outputs. The goal is to make predictions for new, unseen inputs based on the training data.

    Unsupervised learning is a machine learning paradigm where the algorithm is trained on an unlabeled dataset to discover hidden structure in the data. The goal is to identify patterns or relationships in the data without the guidance of labeled outcomes.

    Weakly supervised learning is a type of learning where the algorithm is trained on incomplete or partially labeled data. The goal is to still learn a useful mapping function with limited label information.

    Semi-supervised learning is a type of learning that lies between supervised and unsupervised learning. The algorithm is trained on a mixture of labeled and unlabeled data. The goal is to make use of both labeled and unlabeled data to improve the quality of the learned function.

    Active learning is a type of learning where the algorithm can actively query for labels on instances in order to improve its performance. The goal is to minimize the amount of labeled data needed for the algorithm to achieve a desired level of performance.

2. Empirical risk minimization.
    1. [E] What’s the risk in empirical risk minimization?
    

        *Answer:* the average loss over  a finite dataset D (drawn from p). It's the average error on the examples in this dataset
    2. [E] Why is it empirical?
    

        *Answer:* It's an estimate of expected risk.
    3. [E] How do we minimize that risk?

        *Answer:* ERM minimization principle: we train a model (we adapt its parameters) so that it makes a minimum of errors on the training set, and we evaluate on a seperated test set.

3. [E] Occam's razor states that when the simple explanation and complex explanation both work equally well, the simple explanation is usually correct.  How do we apply this principle in ML?

    *Answer:* Occam's razor can be applied in ML by preferring simpler models with fewer parameters or assumptions over complex models with many parameters or assumptions. The idea is that a simpler model is less likely to overfit the data and generalize better to unseen data.

4. [E] What are the conditions that allowed deep learning to gain popularity in the last decade?

    *Answer:*The conditions that allowed deep learning to gain popularity in the last decade include the availability of large amounts of labeled data, the development of more powerful computing hardware, and the discovery of effective training algorithms for deep neural networks. The combination of these factors allowed researchers to train deep neural networks on complex tasks and achieve state-of-the-art performance on a variety of problems.
5. [M] If we have a wide NN and a deep NN with the same number of parameters, which one is more expressive and why?

    *Answer:* 
    A deep neural network is generally considered more expressive than a wide neural network with the same number of parameters. This is because a deep network has more hidden layers and therefore a larger capacity to learn complex representations of the input data. The number of hidden layers and neurons in each layer can be used to control the capacity and expressiveness of the network. However, it is important to note that having a deep network does not guarantee improved performance, and there may be trade-offs with respect to training time and overfitting.




6. [H] The Universal Approximation Theorem states that a neural network with 1 hidden layer can approximate any continuous function for inputs within a specific range. Then why can’t a simple neural network reach an arbitrarily small positive error?

    *Answer:* The Universal Approximation Theorem states that a single hidden layer neural network can approximate any continuous function to an arbitrary degree of accuracy, given a sufficient number of hidden neurons. However, this theorem does not guarantee that a simple neural network can reach an arbitrarily small positive error, as there are several factors that can prevent this from happening.

    Overfitting: A neural network may learn the training data too well, memorizing the training examples instead of learning the underlying pattern. This can result in high training accuracy but poor generalization performance on unseen data.

    Model capacity: If the network is not complex enough, it may not be able to capture the complexity of the data and produce high error. On the other hand, a too complex network may also overfit the data.

    Data distribution: The range of inputs and the distribution of the outputs can also impact the ability of a network to reach an arbitrarily small positive error. For example, if the data has outliers or is highly skewed, the network may struggle to capture the true pattern.

    Non-convex optimization: The optimization problem in training neural networks is non-convex, meaning that there may be multiple local minima that the optimization algorithm can get stuck in, preventing the network from reaching a global minimum and an arbitrarily small positive error.

    In practice, reaching an arbitrarily small positive error often requires a combination of carefully selecting the model architecture, training algorithms, and regularization techniques, as well as having a large and diverse enough dataset.

7. [E] What are saddle points and local minima? Which are thought to cause more problems for training large NNs?
    *Answer:* A saddle point or minimax point is a point on the surface of the graph of a function where the slopes (derivatives) in orthogonal directions are all zero (a critical point), but which is not a local extremum of the function.
    
    Local minima, on the other hand, are points where the loss function has a gradient of zero in all directions. In this case, optimization algorithms will stop at these points, as they are unable to reduce the loss any further.

    Both saddle points and local minima can cause problems for training large neural networks, but local minima are generally considered to be a more significant issue. This is because a network that is stuck in a local minimum will have a suboptimal performance, as the loss function will not be minimized to its full potential. In contrast, a network stuck at a saddle point may still be able to make some progress in reducing the loss, but the optimization process may be slow and unstable.

    To mitigate these issues, various techniques have been developed, such as using alternative optimization algorithms, adding regularization terms to the loss function, and using techniques such as early stopping and weight decay.




8. Hyperparameters.

    4. [E] What are the differences between parameters and hyperparameters?

        *Answers:* 
        Parameters are values within a machine learning model that are learned from data during training, such as weights in a neural network.

        Hyperparameters, on the other hand, are settings for the model that are set before training and control the overall behavior of the model. Examples of hyperparameters include learning rate, number of hidden layers, and regularization strength.
    5. [E] Why is hyperparameter tuning important?
    
        *Answers:* 
        Hyperparameter tuning is important because the choice of hyperparameters can greatly impact the performance of a machine learning model. A well-tuned model will have better generalization performance, meaning that it will perform better on new, unseen data. On the other hand, poorly chosen hyperparameters can result in overfitting or underfitting the data, leading to poor performance on new data.


    6. [M] Explain algorithm for tuning hyperparameters.

        *Answers:* 
        There are several algorithms that can be used to tune hyperparameters, including:

        Grid Search: This method exhaustively tries all combinations of hyperparameters specified in a grid.

        Random Search: This method randomly samples hyperparameters from a specified distribution.

        Bayesian optimization: This method uses a probabilistic model to guide the search for the best hyperparameters. ([TODO]Read More..)

        Gradient-based optimization: This method uses the gradient of the validation loss with respect to the hyperparameters to update the hyperparameters.  ([TODO]Read More..)


        Each of these algorithms has its own strengths and weaknesses, and the choice of algorithm will depend on the specific problem and computational resources available. A common approach is to use a combination of these methods, such as performing a preliminary grid search or random search to narrow down the range of possible hyperparameters, followed by a more fine-grained search using a gradient-based or Bayesian optimization method.





9. Classification vs. regression.
    
    7. [E] What makes a classification problem different from a regression problem?
    
        *Answer:* Classification is a type of machine learning problem where the goal is to predict a categorical label or class for an input data sample.

        Regression, on the other hand, is a type of machine learning problem where the goal is to predict a continuous output value for an input data sample.
    8. [E] Can a classification problem be turned into a regression problem and vice versa?
        *Answer:*
        A classification problem can be turned into a regression problem by converting the categorical labels into real-valued numbers, such as one-hot encoding or other label encoding methods.

        A regression problem can be turned into a classification problem by defining a set of threshold values to convert the continuous output into discrete classes.

10. Parametric vs. non-parametric methods.
    
    9. [E] What’s the difference between parametric methods and non-parametric methods? Give an example of each method.

        *Answers:*

        Parametric methods make assumptions about the underlying functional form of the data, such as assuming that the data follows a normal distribution.

        Non-parametric methods do not make such assumptions and instead allow the model to have a flexible number of parameters, such as in decision trees or nearest neighbor methods.
        Example of parametric methods:

        * Linear Regression
        * Logistic Regression

        Example of non-parametric methods:

        * k-Nearest Neighbors (k-NN)
        * Decision Trees
        
    10. [H] When should we use one and when should we use the other?

        *Answer:*
        Parametric methods are often faster and easier to train, as they have a smaller number of parameters to estimate. They are also more interpretable, as the parameters have a direct meaning in terms of the underlying relationships between the features and the target.

        Non-parametric methods are often more flexible and can model more complex relationships between the features and the target. They are often used when the true underlying functional form of the data is not known or when the number of parameters required to model the data is large.

        In general, non-parametric methods tend to perform better on smaller datasets, while parametric methods tend to perform better on larger datasets.


11. [M] Why does ensembling independently trained models generally improve performance?

    *Answer:* Ensembling combines the predictions of multiple independently trained models to produce a single, more accurate prediction. This can be done through techniques like majority voting for classification problems or taking the average for regression problems. Ensembling helps reduce overfitting and improve generalization, as the individual models may capture different patterns in the data, and the combination of these patterns can lead to a more robust and accurate prediction.

12. [M] Why does L1 regularization tend to lead to sparsity while L2 regularization pushes weights closer to 0?

    *Answer:* 
    L1 regularization, also known as Lasso, adds a penalty term to the cost function that is proportional to the absolute value of the weights. This leads to sparse solutions, where some of the weights are exactly 0, effectively reducing the number of features used by the model.

    L2 regularization, also known as Ridge, adds a penalty term to the cost function that is proportional to the square of the weights. This leads to solutions where the weights are small but non-zero, effectively reducing the magnitude of the weights.

13. [E] Why does an ML model’s performance degrade in production?

    *Answer:*

    Model performance degradation in production:
    There can be several reasons why a model's performance may degrade in production, such as:

    * Overfitting to the training data, which can cause poor generalization to unseen data.
    
    * Lack of adaptation to changes in the distribution of the data, such as changes in the distribution of features over time.
    
    * Incomplete or incorrect data, which can result in bias in the model's predictions.
    
    * Insufficient validation and testing, which can result in models that are poorly calibrated or overfit to the validation data.

14. [M] What problems might we run into when deploying large machine learning models?

    *Answer:*
    There can be several challenges when deploying large machine learning models, such as:
    
    * High computational and memory requirements, which can make it difficult to run the models in real-time.

    * Lack of interpretability, which can make it difficult to understand the model's predictions and debug any errors.
    
    * Model drift, which can occur when the distribution of the data changes over time, leading to a decrease in the model's performance.
    
    * Privacy and security concerns, which can arise when deploying models that use sensitive data, such as personal information or financial data.

15. Your model performs really well on the test set but poorly in production.

    11. [M] What are your hypotheses about the causes?

        *Answer:*
        The model's poor performance in production could be due to several reasons such as:

        Overfitting to the test data, leading to poor generalization to unseen data in production.
        
        Data shift, meaning that the distribution of the data in production is different from the distribution of the data used to train the model.
        
        Incorrect or missing feature engineering, causing the model to rely on irrelevant or noisy features

    12. [H] How do you validate whether your hypotheses are correct?

        *Answer:*
        Collect and analyze data from both the test set and production environment to compare the data distributions and identify any significant differences.

        Evaluate the model's performance on new, unseen data and compare it with the performance on the test set.
        
        Analyze the feature importances and coefficients of the model to see if the model is relying on irrelevant or noisy features.
    
    13. [M] Imagine your hypotheses about the causes are correct. What would you do to address them?
    
        *Answer:*
        Use techniques like cross-validation, early stopping, and regularization to prevent overfitting and improve the model's generalization ability.

        Re-balance the training data to match the distribution of the data in the production environment.

        Re-engineer the features and/or collect more relevant and representative data to provide the model with better information to make predictions.

